{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_h2h_urls(url):\n",
    "    \"\"\"\n",
    "    Scrape head-to-head URLs from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to scrape.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Use ScraperAPI to bypass potential blocks\n",
    "    payload = {\n",
    "        'api_key': 'b3349e3c5ce9f2853b2b8cee2c0052a7',\n",
    "        'url': url\n",
    "    }\n",
    "\n",
    "    response = requests.get('https://api.scraperapi.com/', params=payload)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Request successful. Status code: {response.status_code}\")\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the standings table\n",
    "        standings_table = soup.select_one('table.stats_table')\n",
    "\n",
    "        if standings_table:\n",
    "            links = standings_table.find_all('a')\n",
    "\n",
    "            # Print out the links found for debugging\n",
    "            print(f\"Total links found: {len(links)}\")\n",
    "\n",
    "            h2h_urls = [f\"https://fbref.com{l.get('href')}\"\n",
    "                        for l in links if l.get(\"href\") and '/stathead/matchup/teams/' in l.get(\"href\")]\n",
    "\n",
    "            # Print out the h2h_urls found for debugging\n",
    "            print(f\"Total head-to-head URLs found: {len(h2h_urls)}\")\n",
    "\n",
    "            directory = '/Users/joathcarrera/Desktop/CSE115A/Soccer-Match-Predictor/H2H_Stats/Urls'\n",
    "            filename = 'urls.csv'\n",
    "            filepath = os.path.join(directory, filename)\n",
    "\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "            # Save the URLs to a CSV file in the specified location\n",
    "            with open(filepath, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['URL'])  # Writing the header\n",
    "                for url in h2h_urls:\n",
    "                    writer.writerow([url])\n",
    "\n",
    "            print(f\"CSV file saved to: {filepath}\")\n",
    "        else:\n",
    "            print(\"Standings table not found in the page.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
