{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Importing the os module for file operations\n",
    "import csv  # Importing the csv module for CSV file handling\n",
    "\n",
    "import requests  # Importing the requests module for HTTP requests\n",
    "from bs4 import BeautifulSoup  # Importing BeautifulSoup for HTML parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_h2h_urls(url):\n",
    "    \n",
    "    \"\"\"\n",
    "    Scrape head-to-head URLs from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url: The URL to scrape.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use ScraperAPI to bypass potential blocks\n",
    "    payload = {\n",
    "        'api_key': 'b3349e3c5ce9f2853b2b8cee2c0052a7',\n",
    "        'url': url\n",
    "    }\n",
    "\n",
    "    # Send a GET request to ScraperAPI with the provided URL\n",
    "    response = requests.get('https://api.scraperapi.com/', params=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the standings table in the parsed HTML\n",
    "        standings_table = soup.select_one('table.stats_table')\n",
    "\n",
    "\n",
    "        if standings_table:\n",
    "            # Find all links in the standings table\n",
    "            links = standings_table.find_all('a')\n",
    "\n",
    "            # Filter and construct head-to-head URLs from the links\n",
    "            h2h_urls = [f\"https://fbref.com{l.get('href')}\"\n",
    "                        for l in links if l.get(\"href\") and '/stathead/matchup/teams/' in l.get(\"href\")]\n",
    "\n",
    "            # Define the directory and file path for saving URLs\n",
    "            directory = '/Users/joathcarrera/Desktop/CSE115A/Soccer-Match-Predictor/H2H_Stats/Urls'\n",
    "            filename = 'urls.csv'\n",
    "            filepath = os.path.join(directory, filename)\n",
    "\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "            # Save the URLs to a CSV file in the specified location\n",
    "            with open(filepath, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['URL']) \n",
    "                for url in h2h_urls:\n",
    "                    writer.writerow([url])\n",
    "\n",
    "        else:\n",
    "            print(\"Standings table not found in the page.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
